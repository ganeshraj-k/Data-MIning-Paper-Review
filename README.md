# Deep Neural Networks: An Experimental Study of Generalization

This project is an implementation and review of the paper "An Experimental Study of What Makes Deep Neural Networks Generalize Well - The Role of Regularization".

## 🎯 Goal

The goal of this project is to gain a deeper understanding of the factors that influence the generalization performance of deep neural networks. We investigate the effects of explicit regularization methods like weight decay and dropout by performing image classification on the Fashion MNIST datasets, and explore implicit procedures like SGD theoretically.

## 📚 Dataset

The dataset used for this project is the Fashion MNIST dataset, a collection of 70,000 grayscale images of fashion items, including ten different categories, such as t-shirts, dresses, sneakers, and sandals.

## 📝 Theorem

We explore the Vapnik-Chervonenkis (VC) Dimension theory, which gives a probabilistic upper bound on test error. It hints that neural networks possess the potential to memorize any kind of dataset fed to them.

## 🛠️ Regularization Techniques

We implement regularization techniques like weight decay (L2 Regularization) and dropout on the models to understand the effect of regularization on generalization.

## 📈 Methodology

We perform experiments with three different neural network models and explore the effect on the generalization error. The impact of model architectures and regularization techniques are inspected in the framework built.

## 📊 Results

The results of our empirical evaluation of models are presented, highlighting the mutual influence of world events and companies. In-depth sector-wise analysis reveals varying impacts of events on different sectors. Pfizer's future trend is forecasted using ARIMA.

## 👥 Contributors

This project is the result of the hard work and dedication of the following individuals:
- Ganesh Raj Kyatham (grk62)
- Manisha Gayatri Damera (md1723)
- Neha Thonta (nt446)

Thank you for visiting this repository!
